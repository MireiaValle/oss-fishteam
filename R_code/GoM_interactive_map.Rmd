---
title: "GoM_interactive_map"
author: "Tang"
date: "July 24, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(stringr)
library(dplyr)
library(parallel)
library(plyr)
library(pryr)
library(DT)
library(pbmcapply)

```

# Raw data file example
The data was downloaded from https://www.nodc.noaa.gov/OC5/WOD/pr_wod.html using  the WOD select tool to grab data(latitude 18.5 - 32.5, longitude -97.8 - -80.0) using all defaults (selected all variables from all data sources/years for the entire GOM). Data were downloaded in standard .csv format. We chose observed level data, and applied the most recently published XBT correction (Cheng et al. 2014).
```{r echo=FALSE}
setwd("C:/Users/yiyit/Google Drive/nceas_workshop/")
rawdata<-readLines("ocldb1499972358.22646.MRB.csv")
rawdata[1:20]
```
And, we have four of such huge tables, each containing more than 20,000 tables.

# Step 1, extract date and location information
We want to create a table in the following format
```{r echo = FALSE}
sampledata<-read.csv(file="loc_date_ctd.csv",nrows=10)
datatable(sampledata)
```
To do that, based on the structure of available data, we first extract the location, date and cast data using `str_match`. We want `cast`, becasue, this is the unique ID we can use later to join the environment measurements data.
```{r eval=FALSE, echo=FALSE}
# Select the all the cast, latitudes, longtitude and date data from the csv file
lat<-str_match(data.ctd,"(Latitude)\\s+,,\\s*(\\S\\d+.\\d*)")
lon<-str_match(data.ctd,"(Longitude)\\s+,,\\s*(\\S\\d+.\\d*)")
year<-str_match(data.ctd,"(Year)\\s+,,\\s*(\\d+)")
month<-str_match(data.ctd,"(Month)\\s+,,\\s*(\\d+)")
day<-str_match(data.ctd,"(Day)\\s+,,\\s*(\\d+)")
hour<-str_match(data.ctd,"(Time)\\s+,,\\s*(\\d+)")
cast<-str_match(data.ctd,"(CAST)\\s+,,\\s*(\\d+)")

# Save the latitude data in a matrix with col one lat and col 2 the value of the lat
lat.df<- data.frame(na.omit(lat[,-1:-2]),stringsAsFactors=FALSE)
lon.df<- data.frame(na.omit(lon[,-1:-2]),stringsAsFactors=FALSE)
year.df<-data.frame(na.omit(year[,-1:-2]),stringsAsFactors=FALSE)
month.df<-data.frame(na.omit(month[,-1:-2]),stringsAsFactors=FALSE)
day.df<-data.frame(na.omit(day[,-1:-2]),stringsAsFactors=FALSE)
hour.df<-data.frame(na.omit(hour[,-1:-2]),stringsAsFactors=FALSE)
cast.df<-data.frame(na.omit(cast[,-1:-2]),stringsAsFactors=FALSE)
colnames(lat.df)<-c("lat")
colnames(lon.df)<-c("lon")
colnames(year.df)<-c("year")
colnames(month.df)<-c("month")
colnames(day.df)<-c("day")
colnames(hour.df)<-c("hour")
colnames(cast.df)<-c("cast")

# Combine the cast, location and date data into one table
loc.date.cdt<-bind_cols(cast.df,lat.df,lon.df,year.df,month.df,day.df)
latdir<-getwd()
 write_csv(loc.date.cdt,file.path(dir,"loc_date_cdt.csv"))

```
Using this process we obtain data in the following format
```{r echo = FALSE}
datatable(read.csv(file="loc_date_ctd.csv",nrows=10))
```


# Step 2, extract all tables with environmental measurement to read
Insepecting the raw data table, we find the rows we need always start with `VARIABLES` and end with `END OF VARIALBE SECTION`. Thus, we use this two string and `str_detect` to find the beginning and ending row index for each table we want to read. Then, creating a loop to extract all tables with measurements. Finally, we also assign the cast ID for each table.

We have some code to extract tables from CTD, XBT, and PFL casts:

```{r eval= FALSE, echo=FALSE}

mc <- getOption("mc.cores", 20)

extract_loc_date=function(data_type="CTD"){
  
csv_file=paste("ocldb1499972358.22646.",data_type,".csv",sep="")
char_lines=readLines(csv_file)

#Make table with cast location and time information called loc_date

lat<-str_match(char_lines,"(Latitude)\\s+,,\\s+(\\d+.\\d+)")
lon<-str_match(char_lines,"(Longitude)\\s+,,\\s+(-\\d+.\\d+)")
year<-str_match(char_lines,"(Year)\\s+,,\\s+(\\d+)")
month<-str_match(char_lines,"(Month)\\s+,,\\s+(\\d+)")
day<-str_match(char_lines,"(Day)\\s+,,\\s+(\\d+)")
cast<-str_match(char_lines,"(CAST)\\s+,,\\s+(\\d+)")
# Save the latitude data in a matrix with col one lat and col 2 the value of the lat
lat_df<- data.frame(na.omit(lat[,-1:-2]),stringsAsFactors=FALSE)
lon_df<- data.frame(na.omit(lon[,-1:-2]),stringsAsFactors=FALSE)
year_df<-data.frame(na.omit(year[,-1:-2]),stringsAsFactors=FALSE)
month_df<-data.frame(na.omit(month[,-1:-2]),stringsAsFactors=FALSE)
day_df<-data.frame(na.omit(day[,-1:-2]),stringsAsFactors=FALSE)
cast_df<-data.frame(na.omit(cast[,-1:-2]),stringsAsFactors=FALSE)
colnames(lat_df)<-c("lat")
colnames(lon_df)<-c("lon")
colnames(year_df)<-c("year")
colnames(month_df)<-c("month")
colnames(day_df)<-c("day")
colnames(cast_df)<-c("cast")

# Combine the cast, location and date data into one table
loc_date<-cbind(cast_df,lat_df,lon_df,year_df,month_df,day_df)
return(loc_date)
}
  

########################################################################
extract_tables=function(data_type="CTD"){
  
csv_file=paste("ocldb1499972358.22646.",data_type,".csv",sep="")
char_lines=readLines(csv_file)
  
#Extract beginning and ending lines for each table

beg=which(str_detect(char_lines, "^VARIABLES*"))
end=which(str_detect(char_lines, "END OF VARIABLES*"))
casts=which(str_detect(char_lines, "^CAST*"))
length(beg)

########################################################################

#Extract each table and add unique id column (Meas_ID) and cast column (Cast)

#Make a list to store the output temporarily (great for debugging)
table_list=vector("list", length(beg)) 

#Make a function to read in using data.tables() in case there is an error in fread()
alternative_read=function(i,col_numbers=col_numbers){
  alt_data=read.table(csv_file,skip=beg[i]+2,
                      nrows=end[i]-beg[i]-3,sep=',',header=FALSE)
  alt_data=alt_data[,col_numbers]
  return(alt_data)
}

#Record system time
t = Sys.time()
data_extract=mclapply(1:length(beg),function(i){
  
  #Figure out how many columns
  ncol_raw_data=length(fread(csv_file,skip=beg[i]+2,
                             nrows=1,sep=',',header=FALSE))
  
  #Store which ones to extract
  col_numbers=c(1,seq(2,ncol_raw_data-1,3))
  
  #Pulls out the data
  data=tryCatch(fread(csv_file,skip=beg[i]+2,
                      nrows=end[i]-beg[i]-3,sep=',',header=FALSE,
                      select=col_numbers),error = function(e) alternative_read(i,col_numbers))
  
  #Pulls out the header
  header=stringi::stri_extract_all_words(char_lines[beg[i]])[[1]][seq(2,ncol_raw_data-1,3)]
  
  #Adds the cast number
  data$Cast=rep(str_extract(char_lines[casts[i]],"[0-9]+"),nrow(data))
  
  #Adds header names
  names(data)=c("Meas_ID",header,"Cast")
  
  return(data)
})
print(Sys.time()-t) #Prints out run time

return(data_extract)
}

########################################################################

# Save table lists

loc_date_CTD=extract_loc_date()
table_list_CTD=extract_tables()

loc_date_XBT=extract_loc_date(data_type="XBT")
table_list_XBT=extract_tables(data_type="XBT")

loc_date_PFL=extract_loc_date(data_type="PFL")
table_list_PFL=extract_tables(data_type="PFL")

save(table_list_CTD, file="./table_list_CTD.Rdata") #saves variable table_list to file
save(table_list_XBT, file="./table_list_XBT.Rdata") 
save(table_list_PFL, file="./table_list_PFL.Rdata") 

load("table_list_CTD.Rdata")
load("table_list_XBT.Rdata")
load("table_list_PFL.Rdata")


########################################################################
```

For OSD specifically, we used a different process due to many rows missing environmental data.

```{r eval= FALSE, echo=FALSE}
### For OSD specifically, we used a different process due to many rows missing environmental data.

#Make table with cast location and time information called loc_date

char_lines=readLines("ocldb1499972358.22646.OSD.csv")

########################################################################

#Extract beginning and ending lines for each table

beg=which(str_detect(char_lines, "^VARIABLES*"))
end=which(str_detect(char_lines, "END OF VARIABLES*"))
L<-length(beg)

casts<-vector()
casts[1]<-cast_df$cast[1]
casts[2]<-cast_df$cast[2]
for (i in (3:L)){
  
  casts[i]=max(which(str_detect(char_lines[(beg[i]-50):(beg[i])], "CAST")))+beg[i]-51
  castcatch<-str_match(char_lines[as.numeric(casts[i])],"(CAST)\\s+,,\\s*(\\d*)")
  casts[i]<-as.numeric(castcatch[,3])
  
}
measure<- data.frame(flag=integer(),
                     Depth=character(), 
                     Temperatur=character(),
                     Cast=integer(),
                     stringsAsFactors=FALSE)
table_list_CSV=vector("list", length(beg))

Seq<-1:L

table_list <- pbmclapply(1:1000, function(i){
    
    #Figure out how many columns
    ncol_raw_data=length(read.table("ocldb1499972358.22646.XBT.csv",skip=beg[i]+2,
                                    nrows=end[i]-beg[i]-3,sep=',',header=FALSE))
    
    #Store which ones to extract
    col_numbers=c(1,seq(2,ncol_raw_data-1,3))
    
    #Pulls out the data
    data<-read.table("ocldb1499972358.22646.XBT.csv",skip=beg[i]+2,
                     nrows=end[i]-beg[i]-3,sep=',',header=FALSE)
    data=data[,col_numbers]
    
    
    #Pulls out the header
    header=stringi::stri_extract_all_words(char_lines[beg[i]])[[1]][seq(2,ncol_raw_data-1,3)]
    
    #Adds the cast number
    data$Cast=rep(casts[i],times=nrow(data))
    
    #Adds header names
    names(data)=c("flag",header,"Cast")
    
    data
    #measure<-rbind.fill(measure,data)
    
  })
  

cat("Done\n")
for (i in Seq){
  measure=rbind.fill(measure,table_list[[1]])
}
write.csv(measure, "env_measures_xbt.csv") #saves variable table_list to file

```
We obtain one table for each data set with all oceanographic measurement and cast ID as below
```{r echo=FALSE}
datatable(read.csv(file="env_measure_all.csv",nrows=10))

```
Then, we combine the four tables into one big table with all oceanographic measurments from all different methods.

```{r eval= FALSE, echo=FALSE}
########################################################################

# Combine each list of tables into one list, using fill=TRUE to add NAs to missing variables

NOAA_env_data_CTD=rbindlist(table_list_CTD,fill=TRUE)
NOAA_env_data_XBT=rbindlist(table_list_XBT,fill=TRUE)
NOAA_env_data_PFL=rbindlist(table_list_PFL,fill=TRUE)

# Assign cast type to each combined list

NOAA_env_data_CTD$Type=rep("CTD",nrow(NOAA_env_data_CTD))
NOAA_env_data_XBT$Type=rep("XBT",nrow(NOAA_env_data_XBT))
NOAA_env_data_PFL$Type=rep("PFL",nrow(NOAA_env_data_PFL))

# Combine all tables together

NOAA_env_data=rbind(NOAA_env_data_CTD,NOAA_env_data_XBT,fill=TRUE)
NOAA_env_data=rbind(NOAA_env_data,NOAA_env_data_PFL,fill=TRUE)

# Checking the class of each column and saving
str(NOAA_env_data)
head(NOAA_env_data)

save(NOAA_env_data, file="NOAA_env_data.Rdata") 

load("NOAA_env_data.Rdata")

# Convert each column to numeric

NOAA_env_data[,2] <- lapply(NOAA_env_data[,2], function(x) as.numeric(as.character(x)))
NOAA_env_data[,3] <- lapply(NOAA_env_data[,3], function(x) as.numeric(as.character(x)))
NOAA_env_data[,4] <- lapply(NOAA_env_data[,4], function(x) as.numeric(as.character(x)))
NOAA_env_data[,5] <- lapply(NOAA_env_data[,5], function(x) as.numeric(as.character(x)))
NOAA_env_data[,6] <- lapply(NOAA_env_data[,6], function(x) as.numeric(as.character(x)))
NOAA_env_data[,7] <- lapply(NOAA_env_data[,7], function(x) as.numeric(as.character(x)))
NOAA_env_data[,8] <- lapply(NOAA_env_data[,8], function(x) as.numeric(as.character(x)))
NOAA_env_data[,9] <- lapply(NOAA_env_data[,9], function(x) as.numeric(as.character(x)))
NOAA_env_data[,10] <- lapply(NOAA_env_data[,10], function(x) as.numeric(as.character(x)))

# Save file

save(NOAA_env_data, file="NOAA_env_data.Rdata") 

# Find "bad" casts (tables read in incorrectly, usually because there were
# no environmental data collected)
length(unique(NOAA_env_data$Cast[which(is.na(NOAA_env_data$Depth))]))

bad_data=NOAA_env_data[which(is.na(NOAA_env_data$Temperatur)),]

# Number of bad casts in each data collection type

length(unique(bad_data$Cast[bad_data$Type=="CTD"]))
#844
length(unique(bad_data$Cast[bad_data$Type=="XBT"]))
#1873
length(unique(bad_data$Cast[bad_data$Type=="PFL"]))
#27

# Number of "good" casts
length(unique(NOAA_env_data$Cast[which(!is.na(NOAA_env_data$Temperatur))]))
#71716

# Calculating percentage of "bad" files

(844+1873+27)/((844+1873+27)+71716)
#Casts deleted: 2744
#0.036852

# Throwing out bad files

NOAA_env_data_clean=NOAA_env_data[which(!is.na(NOAA_env_data$Temperatur)),]
nrow(NOAA_env_data_clean)

# Saving

save(NOAA_env_data_clean, file="NOAA_env_data_clean.Rdata") 

write.csv(NOAA_env_data_clean,"NOAA_env_data_clean.csv")

NOAA_env_data_clean=read.csv("NOAA_env_data_clean.csv",sep=",",header=TRUE)

```

# Step 3, locate ocean depth for each cast id and merge with result from step 1
We need oceanographic data from different depth level. For example, we need temperature and salinity from midwater level, dissolved oxygen from bottom water depth. We find the water depth for each cast ID using `get.depth` from `marmap`, and save the ocean bottom depth in the file from step 1.
```{r eval=FALSE,echo=FALSE}
locinfo<-read.csv("loc_date_all.csv")
# first find the range of our data
latlon<-locinfo[,c(2,3)]
map.box[1]<- min(latlon$lon)-2
map.box[2]<-min(latlon$lat)-2
map.box[3]<- max(latlon$lon)+2
map.box[4]<-max(latlon$lat)+2
# grab ocean data from NOAA
geodata<-getNOAA.bathy(lat1 = map.box[4],lat2 = map.box[2],lon1 = map.box[3],lon2 = map.box[1],resolution = 4)
# assign detph for each of of location in latlon from locinfo
depth<-get.depth(geodata,x=latlon$lon, y=latlon$lat, locator = FALSE)

test<-merge(depth,locinfo)
colnames(test)[3]<-"Bottom depth"
testunique<-test[!duplicated(test),]
colnames(testunique)[4]<-"Cast"
write.csv(testunique,"loc_date_depth_all.csv")
```
Then, we can obtain a table with all cast ID and bottom ocean depth.
```{r echo = FALSE}
datatable(read.csv(file="loc_date_depth_all.csv",nrows=10))

```

# Step 4, merge data from step 2 with data from step 3
Now, we combine the location, time and bottom depth data with the environment table by cast ID using `merge` with `all = TURE`. Finally, we arrived at one big table with all information
```{r echo = FALSE}
datatable(read.csv(file="env_loc_date_depth_all.csv",nrows=10))

```

# Step 5, data manipulation.
We, need to assign each cast id one unique row of oceacnographic data. The temparture and salinity for each ID is from midwater, the DO for each ID from bottom water level, and the Chol for each ID from surface water.

```{r echo = FALSE, eval=FALSE}

# Additional QA/QC and aggregating by cast

NOAA.data=read.csv("env_loc_date_depth_all.csv")
head(NOAA.data)
str(NOAA.data)
nrow(NOAA.data)

# Converting columns to numeric
b=as.numeric(as.character(NOAA.data[,6]))
NOAA.data[,6]=b
c=as.numeric(as.character(NOAA.data[,10]))
NOAA.data[,10]=c

# Getting rid of Temp, Sal, DO, and Chl values outside a "normal" range,
# getting rid of any rows with temp=NA, looking at summer months (May-July),
# and keeping only years after 1980.

head(NOAA.data)
NOAA.data$Temperatur[NOAA.data$Temperatur<0]=NA
NOAA.data$Temperatur[NOAA.data$Temperatur>40]=NA

NOAA.data.clean=NOAA.data[!is.na(NOAA.data$Temperatur),]
nrow(NOAA.data.clean)
NOAA.data.clean=NOAA.data.clean[NOAA.data.clean$month<8&NOAA.data.clean$month>4,]
nrow(NOAA.data.clean)
NOAA.data.clean=NOAA.data.clean[NOAA.data.clean$year>=1980,]
nrow(NOAA.data.clean)

NOAA.data.clean$Oxygen[NOAA.data.clean$Oxygen<0]=NA
NOAA.data.clean$Oxygen[NOAA.data.clean$Oxygen>16]=NA

NOAA.data.clean$Chlorophyl[NOAA.data.clean$Chlorophyl<0]=NA
NOAA.data.clean$Chlorophyl[NOAA.data.clean$Chlorophyl>100]=NA


casts=unique(NOAA.data.clean$Cast)
length(casts)

# Aggregating over all casts

final.data.frame=data.frame("cast"=casts,"temp"=rep(NA,length(casts)),
                            "sal"=rep(NA,length(casts)),"do"=rep(NA,length(casts)),
                            "chl"=rep(NA,length(casts)),"lat"=rep(NA,length(casts)),
                            "lon"=rep(NA,length(casts)),"year"=rep(NA,length(casts)),
                            "month"=rep(NA,length(casts)),"day"=rep(NA,length(casts)))
head(final.data.frame)
head(NOAA.data.clean)
str(NOAA.data.clean)

# Grabbing water column means for lat, lon, time, temp, and sal 

cols.left=NOAA.data.clean %>% group_by(Cast) %>% dplyr::summarize(lat=mean(lat,na.rm=TRUE),lon=mean(lon,na.rm=TRUE),year=mean(year,na.rm=TRUE),month=mean(month,na.rm=TRUE),day=mean(day,na.rm=TRUE),temp=mean(Temperatur,na.rm=TRUE),sal=mean(Salinity,na.rm=TRUE))

# Grabbing the oxygen at the bottom of the cast, and chl at the top of the cast

cols.max=NOAA.data.clean %>% group_by(Cast) %>% top_n(1,Depth)
cols.min=NOAA.data.clean %>% group_by(Cast) %>% top_n(-1,Depth)
col.right=cols.max[,c(2,5,8,13,27)]
col.chl=cols.min[,c(2,10)]

# Joining it all together

col.right2=left_join(col.right,col.chl,by="Cast")
predict.data=left_join(cols.left,col.right2,by="Cast")
head(predict.data)

# Units for O2 and Chl

#O2 ml l-1
#Chl ug l-1

# Save final file

write.csv(predict.data,"predict_data.csv")

```

# Step 6, create interactive map
Now, we create an interactive map with layers : oil spill incidents, temprature, salinity, DO, and Chol for future prediction of hot spot.


```{r echo = FALSE, eval=FALSE}
library(rgdal)
library(leaflet)
library(raster)
library(rgeos)
library(mapview)

##Grab Gulf of Mexico States
states <- readOGR("cb_2016_us_state_20m/cb_2016_us_state_20m.shp",
                  layer = "cb_2016_us_state_20m", GDAL1_integer64_policy = TRUE)
gom_states <- subset(states, states$STUSPS %in% c(
  "TX", "LA", "MS", "AL", "GA", "FL"
))
#Read in the oil spill in Gulf of mexico info
oilspill<-read.csv("GoM_oilspill.csv")
testall<-read.csv("env_loc_date_depth_all.csv")

temp<-data.frame(temprat=testall$Temperatur,lati=testall$lat,lont=testall$lon)
temp<-temp[!is.na(temp$temprat),]

pal <- colorNumeric(
  palette = "YlOrRd",
  domain = gom_states$ALAND
)
GoM <- leaflet() %>%
  addTiles() %>% 
  addCircles(data=oilspill,
              lng= ~lon,
             lat= ~lat,
             weight= 1,
             fillColor= ~colorQuantile("YlOrRd", max_ptl_release_gallons)(max_ptl_release_gallons),
             radius = ~max_ptl_release_gallons* 1e-4,
               popup = ~ as.character(max_ptl_release_gallons),
             
             group = "oil spill")%>%
   addLayersControl(
    overlayGroups = c("oil spill"),
    options = layersControlOptions(collapsed = FALSE)
  )

 
GoM            

GoM_temp <- leaflet() %>%
  addTiles() %>% 
  addCircles(data=temp,
              lng= ~lont,
             lat= ~lati,
             weight= 1,
             radius = ~temprat* 1e-1,
               popup = ~ as.character(temprat),
             group = "Temprature")%>%
   addLayersControl(
    overlayGroups = c("Temprature"),
    options = layersControlOptions(collapsed = FALSE)
  )
GoM_temp

  
```


